{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and download necessary packages + load dataset for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages to work with datasets for training classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Packages to pre-process data\n",
    "import re\n",
    "import string\n",
    "import nltk  #Natural Language Tool Kit\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Packages to embed the datapoints\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Packages and models for classification task\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling1D, Embedding, GlobalMaxPooling1D\n",
    "from keras.models import clone_model\n",
    "\n",
    "# Packages for summarisation\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel('privacy2_modified.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\killt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\killt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\killt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK datasets for pre-processing\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Loading necessary tools for pre-processing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Pre-proccessing\n",
    "def preprocess_policy(policy):\n",
    "    policy = policy.lower() # Lower case the datapoint\n",
    "    policy = re.sub('[%s]' % re.escape(string.punctuation), '', policy) # Remove special characters\n",
    "    policy = re.sub('\\w*\\d\\w*', '', policy) # Remove unmeaning words such as 123, a1b\n",
    "    tokens = word_tokenize(policy)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    # tokens = [stemmer.stem(token) for token in tokens] #No need Stemming due to policy's nature\n",
    "    # Stemming in this case can lead to over stemming\n",
    "    return tokens\n",
    "\n",
    "# Apply \"preprocess_policy()\" for the training datasets\n",
    "df['tokens'] = df['text'].apply(preprocess_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google News Word2Vec Model - Word embedding method\n",
    "embedding_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Create Vector Representations for Policies\n",
    "def get_average_word2vec(preprocessed_datapoint, w2v_model, generate_missing=False, k=300):\n",
    "    if len(preprocessed_datapoint)<1:\n",
    "        return np.zeros(k)\n",
    "    \n",
    "    # Assign vector value if token is not in model: depends on generate_missing, = 0 in this case\n",
    "    if generate_missing:\n",
    "        vectorized = [w2v_model[token] if token in w2v_model else np.random.rand(k) for token in preprocessed_datapoint]\n",
    "    else:\n",
    "        vectorized = [w2v_model[token] if token in w2v_model else np.zeros(k) for token in preprocessed_datapoint]\n",
    "    \n",
    "    # Calculate the average vector of the datapoint \n",
    "    # by dividing sum of values in same axis to the number of token in a datapoint\n",
    "    length_datapoint = len(vectorized)\n",
    "    summed_vector = np.sum(vectorized, axis=0)\n",
    "    averaged_vector = np.divide(summed_vector, length_datapoint)\n",
    "\n",
    "    return averaged_vector\n",
    "\n",
    "def get_word2vec_embeddings(model, data, generate_missing=False):\n",
    "    embeddings = data['tokens'].apply(lambda x: get_average_word2vec(x, model, generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "\n",
    "w2v_data = get_word2vec_embeddings(embedding_model, df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model to classify - split train,test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(w2v_data, df['type'], test_size=0.2, random_state=38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  1.0\n",
      "Accuracy:  0.927007299270073\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.85      0.90       110\n",
      "           1       0.90      0.98      0.94       164\n",
      "\n",
      "    accuracy                           0.93       274\n",
      "   macro avg       0.94      0.91      0.92       274\n",
      "weighted avg       0.93      0.93      0.93       274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest Classifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Model\n",
    "predictions = clf.predict(X_test)\n",
    "train_predictions = clf.predict(X_train)\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_predictions))\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy:  0.9014598540145985\n",
      "Train Accuracy:  0.9095063985374772\n",
      "\n",
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.77      0.86       110\n",
      "           1       0.87      0.99      0.92       164\n",
      "\n",
      "    accuracy                           0.90       274\n",
      "   macro avg       0.92      0.88      0.89       274\n",
      "weighted avg       0.91      0.90      0.90       274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "svm_predictions = svm_clf.predict(X_test)\n",
    "train_predictions = svm_clf.predict(X_train)\n",
    "print(\"SVM Accuracy: \", accuracy_score(y_test, svm_predictions))\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_predictions))\n",
    "print(\"\\nSVM Classification Report:\\n\", classification_report(y_test, svm_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.8857404021937842\n",
      "Logistic Regression Accuracy:  0.9014598540145985\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.78      0.86       110\n",
      "           1       0.87      0.98      0.92       164\n",
      "\n",
      "    accuracy                           0.90       274\n",
      "   macro avg       0.92      0.88      0.89       274\n",
      "weighted avg       0.91      0.90      0.90       274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classifier\n",
    "logistic_clf = LogisticRegression() \n",
    "logistic_clf.fit(X_train, y_train)\n",
    "\n",
    "logistic_predictions = logistic_clf.predict(X_test)\n",
    "train_predictions = logistic_clf.predict(X_train)\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_predictions))\n",
    "print(\"Logistic Regression Accuracy: \", accuracy_score(y_test, logistic_predictions))\n",
    "print(\"\\nLogistic Regression Classification Report:\\n\", classification_report(y_test, logistic_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  1.0\n",
      "Decision Tree Accuracy:  0.8029197080291971\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.75      0.75       110\n",
      "           1       0.83      0.84      0.84       164\n",
      "\n",
      "    accuracy                           0.80       274\n",
      "   macro avg       0.80      0.79      0.79       274\n",
      "weighted avg       0.80      0.80      0.80       274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "dt_predictions = dt.predict(X_test)\n",
    "train_predictions = dt.predict(X_train)\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_predictions))\n",
    "print(\"Decision Tree Accuracy: \", accuracy_score(y_test, dt_predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, dt_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "35/35 [==============================] - 3s 38ms/step - loss: 0.6802 - accuracy: 0.5960 - val_loss: 0.6690 - val_accuracy: 0.5985\n",
      "Epoch 2/5\n",
      "35/35 [==============================] - 1s 28ms/step - loss: 0.6653 - accuracy: 0.5941 - val_loss: 0.6440 - val_accuracy: 0.5985\n",
      "Epoch 3/5\n",
      "35/35 [==============================] - 1s 24ms/step - loss: 0.6202 - accuracy: 0.6792 - val_loss: 0.5836 - val_accuracy: 0.5985\n",
      "Epoch 4/5\n",
      "35/35 [==============================] - 1s 21ms/step - loss: 0.5319 - accuracy: 0.7797 - val_loss: 0.4525 - val_accuracy: 0.8577\n",
      "Epoch 5/5\n",
      "35/35 [==============================] - 1s 22ms/step - loss: 0.4813 - accuracy: 0.8016 - val_loss: 0.4493 - val_accuracy: 0.8248\n",
      "9/9 [==============================] - 0s 9ms/step\n",
      "35/35 [==============================] - 0s 7ms/step\n",
      "Train Accuracy:  0.8281535648994516\n",
      "CNN Accuracy:  0.8248175182481752\n",
      "\n",
      "CNN Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.77      0.78       110\n",
      "           1       0.85      0.86      0.85       164\n",
      "\n",
      "    accuracy                           0.82       274\n",
      "   macro avg       0.82      0.82      0.82       274\n",
      "weighted avg       0.82      0.82      0.82       274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reshape data for CNN\n",
    "X_train_cnn = np.array(X_train).reshape(len(X_train), 300, 1)\n",
    "X_test_cnn = np.array(X_test).reshape(len(X_test), 300, 1)\n",
    "\n",
    "# Build the CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv1D(128, 5, activation='relu', input_shape=(300, 1)))\n",
    "cnn_model.add(MaxPooling1D(5))\n",
    "cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(Dense(1, activation='sigmoid')) # Assuming binary classification\n",
    "\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnn_model.fit(X_train_cnn, y_train, validation_data=(X_test_cnn, y_test), epochs=5, batch_size=32)\n",
    "\n",
    "# Evaluate the CNN model\n",
    "cnn_predictions = (cnn_model.predict(X_test_cnn) > 0.5).astype(\"int32\").flatten()\n",
    "train_predictions = (cnn_model.predict(X_train_cnn) > 0.5).astype(\"int32\").flatten()\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_predictions))\n",
    "print(\"CNN Accuracy: \", accuracy_score(y_test, cnn_predictions))\n",
    "print(\"\\nCNN Classification Report:\\n\", classification_report(y_test, cnn_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation: 1st time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Cross-Validation Accuracy: 0.81 (+/- 0.02)\n",
      "\n",
      "Random Forest Cross-Validation Accuracy: 0.92 (+/- 0.03)\n",
      "\n",
      "SVM Cross-Validation Accuracy: 0.88 (+/- 0.03)\n",
      "\n",
      "Logistic Regression Cross-Validation Accuracy: 0.87 (+/- 0.05)\n",
      "\n",
      "CNN Cross-Validation Accuracy: 0.84 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_cnn(X_train, y_train, X_val, y_val):\n",
    "    model = clone_model(cnn_model)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
    "    _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# Simulate k-fold cross-validation using 5 splits\n",
    "cnn_accuracies = []\n",
    "for _ in range(5):\n",
    "    X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(np.array(w2v_data).reshape(len(w2v_data), 300, 1), df['type'], test_size=0.2)\n",
    "    acc = train_and_evaluate_cnn(X_train_cnn, y_train_cnn, X_val_cnn, y_val_cnn)\n",
    "    cnn_accuracies.append(acc)\n",
    "\n",
    "# For Decision Tree\n",
    "dt_scores = cross_val_score(DecisionTreeClassifier(), w2v_data, df['type'], cv=5, scoring=\"accuracy\")\n",
    "print(f\"Decision Tree Cross-Validation Accuracy: {dt_scores.mean():.2f} (+/- {dt_scores.std() * 2:.2f})\\n\")\n",
    "\n",
    "# For Random Forest\n",
    "rf_scores = cross_val_score(RandomForestClassifier(), w2v_data, df['type'], cv=5, scoring=\"accuracy\")\n",
    "\n",
    "print(f\"Random Forest Cross-Validation Accuracy: {rf_scores.mean():.2f} (+/- {rf_scores.std() * 2:.2f})\\n\")\n",
    "\n",
    "# For SVM\n",
    "svm_scores = cross_val_score(SVC(), w2v_data, df['type'], cv=5, scoring=\"accuracy\")\n",
    "\n",
    "print(f\"SVM Cross-Validation Accuracy: {svm_scores.mean():.2f} (+/- {svm_scores.std() * 2:.2f})\\n\")\n",
    "\n",
    "# For Logistic Regression\n",
    "logistic_scores = cross_val_score(LogisticRegression(), w2v_data, df['type'], cv=5, scoring=\"accuracy\")\n",
    "print(f\"Logistic Regression Cross-Validation Accuracy: {logistic_scores.mean():.2f} (+/- {logistic_scores.std() * 2:.2f})\\n\")\n",
    "\n",
    "# For CNN\n",
    "print(f\"CNN Cross-Validation Accuracy: {np.mean(cnn_accuracies):.2f} (+/- {np.std(cnn_accuracies) * 2:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Random Forest Accuracy after Grid Search:  0.9233576642335767\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "rf_predictions = best_rf.predict(X_test)\n",
    "print(\"Random Forest Accuracy after Grid Search: \", accuracy_score(y_test, rf_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning  - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "{'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
      "SVM after Grid Search: 0.8905109489051095\n"
     ]
    }
   ],
   "source": [
    "# Define a grid of hyperparameters\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.01, 0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid']  # You can also add 'linear', 'poly', 'sigmoid' to test other kernels\n",
    "}\n",
    "\n",
    "# Use grid search with cross-validation\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# Check the best hyperparameters\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Predict and check accuracy\n",
    "svm_predictions = best_svm.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "print(f\"SVM after Grid Search: {svm_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning - Logistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "{'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Logistic Regression Accuracy after Grid Search: 0.8905109489051095\n"
     ]
    }
   ],
   "source": [
    "param_grid_logistic = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['newton-cg','lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "grid_search_logistic = GridSearchCV(LogisticRegression(max_iter=10000), param_grid_logistic, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search_logistic.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_logistic = grid_search_logistic.best_estimator_\n",
    "\n",
    "# Check the best hyperparameters\n",
    "print(grid_search_logistic.best_params_)\n",
    "\n",
    "# Predict and check accuracy\n",
    "logistic_predictions = best_logistic.predict(X_test)\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_predictions)\n",
    "print(f\"Logistic Regression Accuracy after Grid Search: {logistic_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine - tuning Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best Decision Tree Parameters: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Decision Tree Accuracy after Grid Search:  0.8357664233576643\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Decision Tree Parameters: {best_params}\")\n",
    "\n",
    "best_dt = grid_search.best_estimator_\n",
    "dt_predictions = best_dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy after Grid Search: \", accuracy_score(y_test, dt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy:  0.7883211678832117\n",
      "Random Forest Accuracy:  0.9051094890510949\n",
      "Logistic Regression Accuracy:  0.8905109489051095\n",
      "SVM Accuracy:  0.8905109489051095\n",
      "Epoch 1/5\n",
      "35/35 [==============================] - 2s 30ms/step - loss: 0.6807 - accuracy: 0.5914 - val_loss: 0.6694 - val_accuracy: 0.5985\n",
      "Epoch 2/5\n",
      "35/35 [==============================] - 1s 27ms/step - loss: 0.6608 - accuracy: 0.5978 - val_loss: 0.6415 - val_accuracy: 0.5985\n",
      "Epoch 3/5\n",
      "35/35 [==============================] - 1s 27ms/step - loss: 0.6185 - accuracy: 0.6810 - val_loss: 0.5726 - val_accuracy: 0.6131\n",
      "Epoch 4/5\n",
      "35/35 [==============================] - 1s 28ms/step - loss: 0.5329 - accuracy: 0.7925 - val_loss: 0.4565 - val_accuracy: 0.8650\n",
      "Epoch 5/5\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.4892 - accuracy: 0.7943 - val_loss: 0.4506 - val_accuracy: 0.8394\n",
      "9/9 [==============================] - 0s 8ms/step\n",
      "CNN Accuracy:  0.8394160583941606\n"
     ]
    }
   ],
   "source": [
    "# Train Decision Tree Classifier - {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
    "dt = DecisionTreeClassifier(max_depth=None, min_samples_leaf=1, min_samples_split=4,random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_predictions = dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy: \", accuracy_score(y_test, dt_predictions))\n",
    "\n",
    "# Random Forest - {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_leaf=1, min_samples_split=10, random_state=40)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print(\"Random Forest Accuracy: \", accuracy_score(y_test, predictions))\n",
    "\n",
    "# Logistic Regression Classifier -  {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "logistic_clf = LogisticRegression(solver='liblinear', max_iter=10000, C=10, penalty='l2') \n",
    "logistic_clf.fit(X_train, y_train)\n",
    "logistic_predictions = logistic_clf.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy: \", accuracy_score(y_test, logistic_predictions))\n",
    "\n",
    "# SVM Classifier - {'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
    "svm_clf = SVC(kernel='rbf',gamma=1,  probability=True, C=100)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "svm_predictions = svm_clf.predict(X_test)\n",
    "print(\"SVM Accuracy: \", accuracy_score(y_test, svm_predictions))\n",
    "\n",
    "# Reshape data for CNN\n",
    "X_train_cnn = np.array(X_train).reshape(len(X_train), 300, 1)\n",
    "X_test_cnn = np.array(X_test).reshape(len(X_test), 300, 1)\n",
    "\n",
    "# Build the CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv1D(128, 5, activation='relu', input_shape=(300, 1)))\n",
    "cnn_model.add(MaxPooling1D(5))\n",
    "cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(Dense(1, activation='sigmoid')) # Assuming binary classification\n",
    "\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnn_model.fit(X_train_cnn, y_train, validation_data=(X_test_cnn, y_test), epochs=5, batch_size=32)\n",
    "\n",
    "# Evaluate the CNN model\n",
    "cnn_predictions = (cnn_model.predict(X_test_cnn) > 0.5).astype(\"int32\").flatten()\n",
    "print(\"CNN Accuracy: \", accuracy_score(y_test, cnn_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Event - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We collect and use your personal information for as long as reasonably required in order to provide the a Services, which may include:\n",
      "- developing and testing new or updated products and features without your permission;\n",
      "- for internal record keeping and for marketing.\n",
      "-----------\n",
      "When you upload Posted Content, you automatically grant Vervoe an exclusive, royalty-free, perpetual, irrevocable, worldwide licence to use, reproduce, modify, adapt and publish the content in that Posted Content, including a right to sub-licence as necessary for Vervoe to provide and maintain the Technology. You waive any moral rights you may have in the Posted Content.\n",
      "-----------\n",
      "You agree that your Posted Content is not rude, offensive, racist, or inappropriate, and does not contain material that is contrary to any law applicable to you.\n",
      "-----------\n",
      "We are committed to ensuring that the information you provide is secure, and as such we use commercially reasonable endeavors to keep personal information collected through the Site secure. Such endeavours include requesting your username and password to verify your identity before you a grand access to your account.\n",
      "-----------\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "document_content = \"\"\"We collect and use your personal information for as long as reasonably required in order to provide the a Services, which may include:\n",
    "- developing and testing new or updated products and features without your permission;\n",
    "- for internal record keeping and for marketing.\n",
    "\n",
    "When you upload Posted Content, you automatically grant Vervoe an exclusive, royalty-free, perpetual, irrevocable, worldwide licence to use, reproduce, modify, adapt and publish the content in that Posted Content, including a right to sub-licence as necessary for Vervoe to provide and maintain the Technology. You waive any moral rights you may have in the Posted Content.\n",
    "\n",
    "You agree that your Posted Content is not rude, offensive, racist, or inappropriate, and does not contain material that is contrary to any law applicable to you.\n",
    "\n",
    "We are committed to ensuring that the information you provide is secure, and as such we use commercially reasonable endeavors to keep personal information collected through the Site secure. Such endeavours include requesting your username and password to verify your identity before you a grand access to your account.\"\"\"\n",
    "problematic = []\n",
    "\n",
    "\n",
    "def classify_policy(policy_text):\n",
    "    # Preprocess the policy\n",
    "    tokens = preprocess_policy(policy_text)\n",
    "    embedding = get_average_word2vec(tokens, embedding_model)\n",
    "\n",
    "    # Predict using Random Forest\n",
    "    # rf_prediction = clf.predict([embedding])[0]\n",
    "    # return rf_prediction\n",
    "    \n",
    "    # Predict using SVM\n",
    "    svm_prediction = svm_clf.predict([embedding])[0]\n",
    "    return svm_prediction\n",
    "\n",
    "    # Predict using Logistic Regression\n",
    "    # logistic_prediction = logistic_clf.predict([embedding])[0]\n",
    "    # return logistic_prediction\n",
    "\n",
    "    # Predict using Decision Tree\n",
    "    # dt_predictions = dt.predict([embedding])[0]\n",
    "    # return dt_predictions\n",
    "\n",
    "    # Reshape for CNN\n",
    "    # cnn_input = np.array(embedding).reshape(1, 300, 1)\n",
    "    # Predict using CNN\n",
    "    # cnn_prediction = (cnn_model.predict(cnn_input) > 0.5).astype(\"int32\").flatten()[0]\n",
    "    # return cnn_prediction\n",
    "    \n",
    "problematic = []\n",
    "\n",
    "def split_into_paragraphs(document_content):\n",
    "    # Normalize the line breaks\n",
    "    normalized_content = document_content.replace('\\r\\n', '\\n')\n",
    "    # Split the document by double line breaks\n",
    "    chunks = [p.strip() for p in normalized_content.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    paragraphs = []\n",
    "    current_para = \"\"\n",
    "    for chunk in chunks:\n",
    "        # If the chunk starts with any list indicator, append it to the current paragraph\n",
    "        if chunk.startswith(('•', '+', '-')):\n",
    "            current_para += '\\n' + chunk\n",
    "        else:\n",
    "            # If we have content in the current paragraph, store it and start a new one\n",
    "            if current_para:\n",
    "                paragraphs.append(current_para)\n",
    "                current_para = \"\"\n",
    "            current_para = chunk\n",
    "    # Add any remaining content to the paragraphs list\n",
    "    if current_para:\n",
    "        paragraphs.append(current_para)\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "paragraphs = split_into_paragraphs(document_content)\n",
    "for i, paragraph in enumerate(paragraphs, 1):\n",
    "    print(paragraph)\n",
    "    print(\"-----------\")\n",
    "    \n",
    "for paragraph in paragraphs:\n",
    "    predictions = classify_policy(paragraph)\n",
    "    print(predictions)\n",
    "    if predictions == 0:\n",
    "        problematic.append(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Event - Highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\killt\\AppData\\Local\\Temp\\ipykernel_20196\\2223908615.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: #ff0000\">We collect and use your personal information for as long as reasonably required in order to provide the a Services, which may include:\n",
       "- developing and testing new or updated products and features without your permission;\n",
       "- for internal record keeping and for marketing.</span>\n",
       "\n",
       "<span style=\"background-color: #ff0000\">When you upload Posted Content, you automatically grant Vervoe an exclusive, royalty-free, perpetual, irrevocable, worldwide licence to use, reproduce, modify, adapt and publish the content in that Posted Content, including a right to sub-licence as necessary for Vervoe to provide and maintain the Technology. You waive any moral rights you may have in the Posted Content.</span>\n",
       "\n",
       "You agree that your Posted Content is not rude, offensive, racist, or inappropriate, and does not contain material that is contrary to any law applicable to you.\n",
       "\n",
       "We are committed to ensuring that the information you provide is secure, and as such we use commercially reasonable endeavors to keep personal information collected through the Site secure. Such endeavours include requesting your username and password to verify your identity before you a grand access to your account."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def highlight_problematic_sentences(term, problematic_phrases):\n",
    "    if len(problematic_phrases) >= 1:\n",
    "        for phrase in problematic_phrases:\n",
    "            highlighted_phrase = f'<span style=\"background-color: #ff0000\">{phrase}</span>'\n",
    "            term = term.replace(phrase, highlighted_phrase)\n",
    "        display(HTML(term))\n",
    "\n",
    "highlight_problematic_sentences(document_content, problematic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Event - Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.58k/1.58k [00:00<?, ?B/s]\n",
      "C:\\Users\\killt\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\killt\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\killt\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:979: UserWarning: Not enough free disk space to download the file. The expected file size is: 1625.27 MB. The target location C:\\Users\\killt\\.cache\\huggingface\\hub only has 1200.87 MB free disk space.\n",
      "  warnings.warn(\n",
      "C:\\Users\\killt\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:979: UserWarning: Not enough free disk space to download the file. The expected file size is: 1625.27 MB. The target location C:\\Users\\killt\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn\\blobs only has 1200.87 MB free disk space.\n",
      "  warnings.warn(\n",
      "Downloading pytorch_model.bin:   1%|          | 10.5M/1.63G [00:09<24:27, 1.10MB/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\Document (Swin Study)\\NLP-policy-main\\final_code copy.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Document%20%28Swin%20Study%29/NLP-policy-main/final_code%20copy.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m bart \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39msummarization\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfacebook/bart-large-cnn\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Document%20%28Swin%20Study%29/NLP-policy-main/final_code%20copy.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m articles \u001b[39m=\u001b[39m problematic\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Document%20%28Swin%20Study%29/NLP-policy-main/final_code%20copy.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m summ \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\__init__.py:834\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m--> 834\u001b[0m     framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    835\u001b[0m         model,\n\u001b[0;32m    836\u001b[0m         model_classes\u001b[39m=\u001b[39;49mmodel_classes,\n\u001b[0;32m    837\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    838\u001b[0m         framework\u001b[39m=\u001b[39;49mframework,\n\u001b[0;32m    839\u001b[0m         task\u001b[39m=\u001b[39;49mtask,\n\u001b[0;32m    840\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[0;32m    841\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[0;32m    842\u001b[0m     )\n\u001b[0;32m    844\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    845\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\base.py:269\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    264\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    265\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to load the model with Tensorflow.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m     )\n\u001b[0;32m    268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    271\u001b[0m         model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\auto\\auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    564\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m    566\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    567\u001b[0m     )\n\u001b[0;32m    568\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:2929\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2926\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2927\u001b[0m         \u001b[39m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[0;32m   2928\u001b[0m         filename \u001b[39m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[1;32m-> 2929\u001b[0m         resolved_archive_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m   2930\u001b[0m             pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_file_kwargs\n\u001b[0;32m   2931\u001b[0m         )\n\u001b[0;32m   2932\u001b[0m \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[0;32m   2933\u001b[0m     \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[0;32m   2934\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m   2935\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   2936\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[0;32m   2937\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs,\n\u001b[0;32m   2938\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    427\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    430\u001b[0m         path_or_repo_id,\n\u001b[0;32m    431\u001b[0m         filename,\n\u001b[0;32m    432\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    433\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    434\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    435\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    436\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    437\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    438\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    439\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    440\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m    441\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    443\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1431\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[1;32m-> 1431\u001b[0m     http_get(\n\u001b[0;32m   1432\u001b[0m         url_to_download,\n\u001b[0;32m   1433\u001b[0m         temp_file,\n\u001b[0;32m   1434\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1435\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1436\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1437\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[0;32m   1438\u001b[0m     )\n\u001b[0;32m   1440\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1441\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:551\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[0;32m    541\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(…)\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[0;32m    544\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    545\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    549\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[0;32m    550\u001b[0m )\n\u001b[1;32m--> 551\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[0;32m    552\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    553\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin:   1%|          | 10.5M/1.63G [00:26<24:27, 1.10MB/s]"
     ]
    }
   ],
   "source": [
    "bart = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "articles = problematic\n",
    "summ = []\n",
    "\n",
    "for article in articles:\n",
    "  result = bart(article)\n",
    "  summ.append(result)\n",
    "\n",
    "summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "from joblib import dump\n",
    "dump(svm_clf, 'svm_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
