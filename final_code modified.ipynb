{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and download necessary packages + load dataset for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\killt\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Packages to work with datasets for training classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Packages to pre-process data\n",
    "import re\n",
    "import string\n",
    "import nltk  #Natural Language Tool Kit\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Packages to embed the datapoints\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Packages and models for classification task\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling1D, Embedding, GlobalMaxPooling1D\n",
    "from keras.models import clone_model\n",
    "\n",
    "# Packages for summarisation\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel('privacy2_modified.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\killt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\killt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\killt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK datasets for pre-processing\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Loading necessary tools for pre-processing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Pre-proccessing\n",
    "def preprocess_policy(policy):\n",
    "    policy = policy.lower() # Lower case the datapoint\n",
    "    policy = re.sub('[%s]' % re.escape(string.punctuation), '', policy) # Remove special characters\n",
    "    policy = re.sub('\\w*\\d\\w*', '', policy) # Remove unmeaning words such as 123, a1b\n",
    "    tokens = word_tokenize(policy)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    # tokens = [stemmer.stem(token) for token in tokens] #No need Stemming due to policy's nature\n",
    "    # Stemming in this case can lead to over stemming\n",
    "    return tokens\n",
    "\n",
    "# Apply \"preprocess_policy()\" for the training datasets\n",
    "df['tokens'] = df['text'].apply(preprocess_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google News Word2Vec Model - Word embedding method\n",
    "embedding_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Create Vector Representations for Policies\n",
    "def get_average_word2vec(preprocessed_datapoint, w2v_model, generate_missing=False, k=300):\n",
    "    if len(preprocessed_datapoint)<1:\n",
    "        return np.zeros(k)\n",
    "    \n",
    "    # Assign vector value if token is not in model: depends on generate_missing, = 0 in this case\n",
    "    if generate_missing:\n",
    "        vectorized = [w2v_model[token] if token in w2v_model else np.random.rand(k) for token in preprocessed_datapoint]\n",
    "    else:\n",
    "        vectorized = [w2v_model[token] if token in w2v_model else np.zeros(k) for token in preprocessed_datapoint]\n",
    "    \n",
    "    # Calculate the average vector of the datapoint \n",
    "    # by dividing sum of values in same axis to the number of token in a datapoint\n",
    "    length_datapoint = len(vectorized)\n",
    "    summed_vector = np.sum(vectorized, axis=0)\n",
    "    averaged_vector = np.divide(summed_vector, length_datapoint)\n",
    "\n",
    "    return averaged_vector\n",
    "\n",
    "def get_word2vec_embeddings(model, data, generate_missing=False):\n",
    "    embeddings = data['tokens'].apply(lambda x: get_average_word2vec(x, model, generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "\n",
    "w2v_data = get_word2vec_embeddings(embedding_model, df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model to classify - split train,test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(w2v_data, df['type'], test_size=0.2, random_state=38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  1.0\n",
      "Accuracy:  0.9233576642335767\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.84      0.90       110\n",
      "           1       0.90      0.98      0.94       164\n",
      "\n",
      "    accuracy                           0.92       274\n",
      "   macro avg       0.93      0.91      0.92       274\n",
      "weighted avg       0.93      0.92      0.92       274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest Classifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Model\n",
    "predictions = clf.predict(X_test)\n",
    "train_predictions = clf.predict(X_train)\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_predictions))\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy:  0.9014598540145985\n",
      "Train Accuracy:  0.9095063985374772\n",
      "\n",
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.77      0.86       110\n",
      "           1       0.87      0.99      0.92       164\n",
      "\n",
      "    accuracy                           0.90       274\n",
      "   macro avg       0.92      0.88      0.89       274\n",
      "weighted avg       0.91      0.90      0.90       274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "svm_predictions = svm_clf.predict(X_test)\n",
    "train_predictions = svm_clf.predict(X_train)\n",
    "print(\"SVM Accuracy: \", accuracy_score(y_test, svm_predictions))\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_predictions))\n",
    "print(\"\\nSVM Classification Report:\\n\", classification_report(y_test, svm_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.8857404021937842\n",
      "Logistic Regression Accuracy:  0.9014598540145985\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.78      0.86       110\n",
      "           1       0.87      0.98      0.92       164\n",
      "\n",
      "    accuracy                           0.90       274\n",
      "   macro avg       0.92      0.88      0.89       274\n",
      "weighted avg       0.91      0.90      0.90       274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classifier\n",
    "logistic_clf = LogisticRegression() \n",
    "logistic_clf.fit(X_train, y_train)\n",
    "\n",
    "logistic_predictions = logistic_clf.predict(X_test)\n",
    "train_predictions = logistic_clf.predict(X_train)\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_predictions))\n",
    "print(\"Logistic Regression Accuracy: \", accuracy_score(y_test, logistic_predictions))\n",
    "print(\"\\nLogistic Regression Classification Report:\\n\", classification_report(y_test, logistic_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  1.0\n",
      "Decision Tree Accuracy:  0.8357664233576643\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.78       110\n",
      "           1       0.84      0.90      0.87       164\n",
      "\n",
      "    accuracy                           0.84       274\n",
      "   macro avg       0.83      0.82      0.83       274\n",
      "weighted avg       0.84      0.84      0.83       274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "dt_predictions = dt.predict(X_test)\n",
    "train_predictions = dt.predict(X_train)\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_predictions))\n",
    "print(\"Decision Tree Accuracy: \", accuracy_score(y_test, dt_predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, dt_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "35/35 [==============================] - 2s 28ms/step - loss: 0.6752 - accuracy: 0.5951 - val_loss: 0.6671 - val_accuracy: 0.5985\n",
      "Epoch 2/5\n",
      "35/35 [==============================] - 1s 25ms/step - loss: 0.6648 - accuracy: 0.5960 - val_loss: 0.6494 - val_accuracy: 0.7445\n",
      "Epoch 3/5\n",
      "35/35 [==============================] - 1s 24ms/step - loss: 0.6153 - accuracy: 0.7166 - val_loss: 0.5503 - val_accuracy: 0.7810\n",
      "Epoch 4/5\n",
      "35/35 [==============================] - 1s 21ms/step - loss: 0.4973 - accuracy: 0.8007 - val_loss: 0.4224 - val_accuracy: 0.8686\n",
      "Epoch 5/5\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 0.4275 - accuracy: 0.8391 - val_loss: 0.3877 - val_accuracy: 0.8723\n",
      "9/9 [==============================] - 0s 7ms/step\n",
      "35/35 [==============================] - 0s 6ms/step\n",
      "Train Accuracy:  0.8528336380255942\n",
      "CNN Accuracy:  0.8722627737226277\n",
      "\n",
      "CNN Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.71      0.82       110\n",
      "           1       0.83      0.98      0.90       164\n",
      "\n",
      "    accuracy                           0.87       274\n",
      "   macro avg       0.90      0.85      0.86       274\n",
      "weighted avg       0.89      0.87      0.87       274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reshape data for CNN\n",
    "X_train_cnn = np.array(X_train).reshape(len(X_train), 300, 1)\n",
    "X_test_cnn = np.array(X_test).reshape(len(X_test), 300, 1)\n",
    "\n",
    "# Build the CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv1D(128, 5, activation='relu', input_shape=(300, 1)))\n",
    "cnn_model.add(MaxPooling1D(5))\n",
    "cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(Dense(1, activation='sigmoid')) # Assuming binary classification\n",
    "\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnn_model.fit(X_train_cnn, y_train, validation_data=(X_test_cnn, y_test), epochs=5, batch_size=32)\n",
    "\n",
    "# Evaluate the CNN model\n",
    "cnn_predictions = (cnn_model.predict(X_test_cnn) > 0.5).astype(\"int32\").flatten()\n",
    "train_predictions = (cnn_model.predict(X_train_cnn) > 0.5).astype(\"int32\").flatten()\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_predictions))\n",
    "print(\"CNN Accuracy: \", accuracy_score(y_test, cnn_predictions))\n",
    "print(\"\\nCNN Classification Report:\\n\", classification_report(y_test, cnn_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation: 1st time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Cross-Validation Accuracy: 0.81 (+/- 0.03)\n",
      "\n",
      "Random Forest Cross-Validation Accuracy: 0.92 (+/- 0.03)\n",
      "\n",
      "SVM Cross-Validation Accuracy: 0.88 (+/- 0.03)\n",
      "\n",
      "Logistic Regression Cross-Validation Accuracy: 0.87 (+/- 0.05)\n",
      "\n",
      "CNN Cross-Validation Accuracy: 0.84 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_cnn(X_train, y_train, X_val, y_val):\n",
    "    model = clone_model(cnn_model)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
    "    _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# Simulate k-fold cross-validation using 5 splits\n",
    "cnn_accuracies = []\n",
    "for _ in range(5):\n",
    "    X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(np.array(w2v_data).reshape(len(w2v_data), 300, 1), df['type'], test_size=0.2)\n",
    "    acc = train_and_evaluate_cnn(X_train_cnn, y_train_cnn, X_val_cnn, y_val_cnn)\n",
    "    cnn_accuracies.append(acc)\n",
    "\n",
    "# For Decision Tree\n",
    "dt_scores = cross_val_score(DecisionTreeClassifier(), w2v_data, df['type'], cv=5, scoring=\"accuracy\")\n",
    "print(f\"Decision Tree Cross-Validation Accuracy: {dt_scores.mean():.2f} (+/- {dt_scores.std() * 2:.2f})\\n\")\n",
    "\n",
    "# For Random Forest\n",
    "rf_scores = cross_val_score(RandomForestClassifier(), w2v_data, df['type'], cv=5, scoring=\"accuracy\")\n",
    "\n",
    "print(f\"Random Forest Cross-Validation Accuracy: {rf_scores.mean():.2f} (+/- {rf_scores.std() * 2:.2f})\\n\")\n",
    "\n",
    "# For SVM\n",
    "svm_scores = cross_val_score(SVC(), w2v_data, df['type'], cv=5, scoring=\"accuracy\")\n",
    "\n",
    "print(f\"SVM Cross-Validation Accuracy: {svm_scores.mean():.2f} (+/- {svm_scores.std() * 2:.2f})\\n\")\n",
    "\n",
    "# For Logistic Regression\n",
    "logistic_scores = cross_val_score(LogisticRegression(), w2v_data, df['type'], cv=5, scoring=\"accuracy\")\n",
    "print(f\"Logistic Regression Cross-Validation Accuracy: {logistic_scores.mean():.2f} (+/- {logistic_scores.std() * 2:.2f})\\n\")\n",
    "\n",
    "# For CNN\n",
    "print(f\"CNN Cross-Validation Accuracy: {np.mean(cnn_accuracies):.2f} (+/- {np.std(cnn_accuracies) * 2:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Random Forest Accuracy after Grid Search:  0.9124087591240876\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "rf_predictions = best_rf.predict(X_test)\n",
    "print(\"Random Forest Accuracy after Grid Search: \", accuracy_score(y_test, rf_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning  - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "{'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
      "SVM after Grid Search: 0.8905109489051095\n"
     ]
    }
   ],
   "source": [
    "# Define a grid of hyperparameters\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.01, 0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid']  # You can also add 'linear', 'poly', 'sigmoid' to test other kernels\n",
    "}\n",
    "\n",
    "# Use grid search with cross-validation\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# Check the best hyperparameters\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Predict and check accuracy\n",
    "svm_predictions = best_svm.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "print(f\"SVM after Grid Search: {svm_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning - Logistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "{'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Logistic Regression Accuracy after Grid Search: 0.8905109489051095\n"
     ]
    }
   ],
   "source": [
    "param_grid_logistic = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['newton-cg','lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "grid_search_logistic = GridSearchCV(LogisticRegression(max_iter=10000), param_grid_logistic, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search_logistic.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_logistic = grid_search_logistic.best_estimator_\n",
    "\n",
    "# Check the best hyperparameters\n",
    "print(grid_search_logistic.best_params_)\n",
    "\n",
    "# Predict and check accuracy\n",
    "logistic_predictions = best_logistic.predict(X_test)\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_predictions)\n",
    "print(f\"Logistic Regression Accuracy after Grid Search: {logistic_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine - tuning Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best Decision Tree Parameters: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Decision Tree Accuracy after Grid Search:  0.8357664233576643\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Decision Tree Parameters: {best_params}\")\n",
    "\n",
    "best_dt = grid_search.best_estimator_\n",
    "dt_predictions = best_dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy after Grid Search: \", accuracy_score(y_test, dt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy:  0.7883211678832117\n",
      "Random Forest Accuracy:  0.9051094890510949\n",
      "Logistic Regression Accuracy:  0.8905109489051095\n",
      "SVM Accuracy:  0.8905109489051095\n",
      "Epoch 1/5\n",
      "35/35 [==============================] - 2s 27ms/step - loss: 0.6819 - accuracy: 0.5850 - val_loss: 0.6720 - val_accuracy: 0.5985\n",
      "Epoch 2/5\n",
      "35/35 [==============================] - 1s 21ms/step - loss: 0.6699 - accuracy: 0.5941 - val_loss: 0.6566 - val_accuracy: 0.5985\n",
      "Epoch 3/5\n",
      "35/35 [==============================] - 1s 20ms/step - loss: 0.6457 - accuracy: 0.6042 - val_loss: 0.6121 - val_accuracy: 0.5985\n",
      "Epoch 4/5\n",
      "35/35 [==============================] - 1s 24ms/step - loss: 0.5679 - accuracy: 0.7660 - val_loss: 0.4931 - val_accuracy: 0.8577\n",
      "Epoch 5/5\n",
      "35/35 [==============================] - 1s 23ms/step - loss: 0.4574 - accuracy: 0.8245 - val_loss: 0.4211 - val_accuracy: 0.8577\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "CNN Accuracy:  0.8576642335766423\n"
     ]
    }
   ],
   "source": [
    "# Train Decision Tree Classifier - {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
    "dt = DecisionTreeClassifier(max_depth=None, min_samples_leaf=1, min_samples_split=4,random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_predictions = dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy: \", accuracy_score(y_test, dt_predictions))\n",
    "\n",
    "# Random Forest - {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_leaf=1, min_samples_split=10, random_state=40)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print(\"Random Forest Accuracy: \", accuracy_score(y_test, predictions))\n",
    "\n",
    "# Logistic Regression Classifier -  {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "logistic_clf = LogisticRegression(solver='liblinear', max_iter=10000, C=10, penalty='l2') \n",
    "logistic_clf.fit(X_train, y_train)\n",
    "logistic_predictions = logistic_clf.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy: \", accuracy_score(y_test, logistic_predictions))\n",
    "\n",
    "# SVM Classifier - {'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
    "svm_clf = SVC(kernel='rbf',gamma=1,  probability=True, C=100)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "svm_predictions = svm_clf.predict(X_test)\n",
    "print(\"SVM Accuracy: \", accuracy_score(y_test, svm_predictions))\n",
    "\n",
    "# Reshape data for CNN\n",
    "X_train_cnn = np.array(X_train).reshape(len(X_train), 300, 1)\n",
    "X_test_cnn = np.array(X_test).reshape(len(X_test), 300, 1)\n",
    "\n",
    "# Build the CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv1D(128, 5, activation='relu', input_shape=(300, 1)))\n",
    "cnn_model.add(MaxPooling1D(5))\n",
    "cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(Dense(1, activation='sigmoid')) # Assuming binary classification\n",
    "\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnn_model.fit(X_train_cnn, y_train, validation_data=(X_test_cnn, y_test), epochs=5, batch_size=32)\n",
    "\n",
    "# Evaluate the CNN model\n",
    "cnn_predictions = (cnn_model.predict(X_test_cnn) > 0.5).astype(\"int32\").flatten()\n",
    "print(\"CNN Accuracy: \", accuracy_score(y_test, cnn_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Event - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We collect and use your personal information for as long as reasonably required in order to provide the a Services, which may include:\n",
      "- developing and testing new or updated products and features without your permission;\n",
      "- for internal record keeping and for marketing.\n",
      "-----------\n",
      "When you upload Posted Content, you automatically grant Vervoe an exclusive, royalty-free, perpetual, irrevocable, worldwide licence to use, reproduce, modify, adapt and publish the content in that Posted Content, including a right to sub-licence as necessary for Vervoe to provide and maintain the Technology. You waive any moral rights you may have in the Posted Content.\n",
      "-----------\n",
      "You agree that your Posted Content is not rude, offensive, racist, or inappropriate, and does not contain material that is contrary to any law applicable to you.\n",
      "-----------\n",
      "We are committed to ensuring that the information you provide is secure, and as such we use commercially reasonable endeavors to keep personal information collected through the Site secure. Such endeavours include requesting your username and password to verify your identity before you a grand access to your account.\n",
      "-----------\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "document_content = \"\"\"We collect and use your personal information for as long as reasonably required in order to provide the a Services, which may include:\n",
    "- developing and testing new or updated products and features without your permission;\n",
    "- for internal record keeping and for marketing.\n",
    "\n",
    "When you upload Posted Content, you automatically grant Vervoe an exclusive, royalty-free, perpetual, irrevocable, worldwide licence to use, reproduce, modify, adapt and publish the content in that Posted Content, including a right to sub-licence as necessary for Vervoe to provide and maintain the Technology. You waive any moral rights you may have in the Posted Content.\n",
    "\n",
    "You agree that your Posted Content is not rude, offensive, racist, or inappropriate, and does not contain material that is contrary to any law applicable to you.\n",
    "\n",
    "We are committed to ensuring that the information you provide is secure, and as such we use commercially reasonable endeavors to keep personal information collected through the Site secure. Such endeavours include requesting your username and password to verify your identity before you a grand access to your account.\"\"\"\n",
    "problematic = []\n",
    "\n",
    "\n",
    "def classify_policy(policy_text):\n",
    "    # Preprocess the policy\n",
    "    tokens = preprocess_policy(policy_text)\n",
    "    embedding = get_average_word2vec(tokens, embedding_model)\n",
    "\n",
    "    # Predict using Random Forest\n",
    "    # rf_prediction = clf.predict([embedding])[0]\n",
    "    # return rf_prediction\n",
    "    \n",
    "    # Predict using SVM\n",
    "    svm_prediction = svm_clf.predict([embedding])[0]\n",
    "    return svm_prediction\n",
    "\n",
    "    # Predict using Logistic Regression\n",
    "    # logistic_prediction = logistic_clf.predict([embedding])[0]\n",
    "    # return logistic_prediction\n",
    "\n",
    "    # Predict using Decision Tree\n",
    "    # dt_predictions = dt.predict([embedding])[0]\n",
    "    # return dt_predictions\n",
    "\n",
    "    # Reshape for CNN\n",
    "    # cnn_input = np.array(embedding).reshape(1, 300, 1)\n",
    "    # Predict using CNN\n",
    "    # cnn_prediction = (cnn_model.predict(cnn_input) > 0.5).astype(\"int32\").flatten()[0]\n",
    "    # return cnn_prediction\n",
    "    \n",
    "problematic = []\n",
    "\n",
    "def split_into_paragraphs(document_content):\n",
    "    # Normalize the line breaks\n",
    "    normalized_content = document_content.replace('\\r\\n', '\\n')\n",
    "    # Split the document by double line breaks\n",
    "    chunks = [p.strip() for p in normalized_content.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    paragraphs = []\n",
    "    current_para = \"\"\n",
    "    for chunk in chunks:\n",
    "        # If the chunk starts with any list indicator, append it to the current paragraph\n",
    "        if chunk.startswith(('â€¢', '+', '-')):\n",
    "            current_para += '\\n' + chunk\n",
    "        else:\n",
    "            # If we have content in the current paragraph, store it and start a new one\n",
    "            if current_para:\n",
    "                paragraphs.append(current_para)\n",
    "                current_para = \"\"\n",
    "            current_para = chunk\n",
    "    # Add any remaining content to the paragraphs list\n",
    "    if current_para:\n",
    "        paragraphs.append(current_para)\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "paragraphs = split_into_paragraphs(document_content)\n",
    "for i, paragraph in enumerate(paragraphs, 1):\n",
    "    print(paragraph)\n",
    "    print(\"-----------\")\n",
    "    \n",
    "for paragraph in paragraphs:\n",
    "    predictions = classify_policy(paragraph)\n",
    "    print(predictions)\n",
    "    if predictions == 0:\n",
    "        problematic.append(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Event - Highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\killt\\AppData\\Local\\Temp\\ipykernel_19668\\2223908615.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: #ff0000\">We collect and use your personal information for as long as reasonably required in order to provide the a Services, which may include:\n",
       "- developing and testing new or updated products and features without your permission;\n",
       "- for internal record keeping and for marketing.</span>\n",
       "\n",
       "<span style=\"background-color: #ff0000\">When you upload Posted Content, you automatically grant Vervoe an exclusive, royalty-free, perpetual, irrevocable, worldwide licence to use, reproduce, modify, adapt and publish the content in that Posted Content, including a right to sub-licence as necessary for Vervoe to provide and maintain the Technology. You waive any moral rights you may have in the Posted Content.</span>\n",
       "\n",
       "You agree that your Posted Content is not rude, offensive, racist, or inappropriate, and does not contain material that is contrary to any law applicable to you.\n",
       "\n",
       "We are committed to ensuring that the information you provide is secure, and as such we use commercially reasonable endeavors to keep personal information collected through the Site secure. Such endeavours include requesting your username and password to verify your identity before you a grand access to your account."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def highlight_problematic_sentences(term, problematic_phrases):\n",
    "    if len(problematic_phrases) >= 1:\n",
    "        for phrase in problematic_phrases:\n",
    "            highlighted_phrase = f'<span style=\"background-color: #ff0000\">{phrase}</span>'\n",
    "            term = term.replace(phrase, highlighted_phrase)\n",
    "        display(HTML(term))\n",
    "\n",
    "highlight_problematic_sentences(document_content, problematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "from joblib import dump\n",
    "dump(svm_clf, 'svm_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
